{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining, an introduction to the Pandas package \n",
    "This is a companion notebook for video content presented as part of the Data Mining course at SMU.\n",
    "\n",
    "In this tutorial we will be looking at a number of different parts of the Pandas package for data analysis, including:\n",
    "- Data Frames\n",
    " - loading data\n",
    " - head and tail commands\n",
    "- Munging\n",
    " - indexing operations\n",
    " - basic statistics\n",
    " - encoding\n",
    " - imputation (optional)\n",
    "- bonus: calling R with magics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Frames in Pandas\n",
    "Data frames in Pandas are basically like tables of data that you can do some really interesting relational database operations upon. There are many built in methods for aggregation and visualization, but we will cover those next time.+\n",
    "\n",
    "## Data Frames in R\n",
    "The data frames in Pandas were designed provide the same data manipulation functionality as data frames within R.  Once you understand the Pandas data frame, you are well on your way to understanding the R data frame.  You can check out the following website for a detailed comparison between data frames using Pandas vs. R:\n",
    "\n",
    "[Data Frames in Pandas vs. R](http://pandas.pydata.org/pandas-docs/stable/comparison_with_r.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load a typical table of data from a csv file. You can download the file from here:\n",
    "https://archive.ics.uci.edu/ml/datasets/Heart+Disease\n",
    "\n",
    "Make sure to place it in this directory or adjust the path for the file.\n",
    "### Reading Data from CSV with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "site,age,is_male,chest_pain,rest_blood_press,cholesterol,high_blood_sugar,rest_ecg,max_heart_rate,exer_angina,ST_depression,Peak_ST_seg,major_vessels,thal,has_heart_disease\n",
      "\n",
      "cleve,63,1,1,145,233,1,2,150,0,2.3,3,0,6,0\n",
      "\n",
      "cleve,67,1,4,160,286,0,2,108,1,1.5,2,3,3,2\n",
      "\n",
      "cleve,67,1,4,120,229,0,2,129,1,2.6,2,2,7,1\n",
      "\n",
      "cleve,37,1,3,130,250,0,0,187,0,3.5,3,0,3,0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Python\n",
    "# let's print out the first five rows inside a csv file\n",
    "\n",
    "# NOTE: you may need to change the path to the file, \n",
    "#       depending on where you saved the data\n",
    "with open('data/heart_disease.csv') as fid:\n",
    "    for idx, row in enumerate(fid):\n",
    "        print (row)\n",
    "        if idx >= 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# now let's read in the same data to save it as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/heart_disease.csv') # read in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site</th>\n",
       "      <th>age</th>\n",
       "      <th>is_male</th>\n",
       "      <th>chest_pain</th>\n",
       "      <th>rest_blood_press</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>high_blood_sugar</th>\n",
       "      <th>rest_ecg</th>\n",
       "      <th>max_heart_rate</th>\n",
       "      <th>exer_angina</th>\n",
       "      <th>ST_depression</th>\n",
       "      <th>Peak_ST_seg</th>\n",
       "      <th>major_vessels</th>\n",
       "      <th>thal</th>\n",
       "      <th>has_heart_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cleve</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cleve</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cleve</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>229</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cleve</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cleve</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    site  age  is_male  chest_pain rest_blood_press cholesterol  \\\n",
       "0  cleve   63        1           1              145         233   \n",
       "1  cleve   67        1           4              160         286   \n",
       "2  cleve   67        1           4              120         229   \n",
       "3  cleve   37        1           3              130         250   \n",
       "4  cleve   41        0           2              130         204   \n",
       "\n",
       "  high_blood_sugar rest_ecg max_heart_rate exer_angina ST_depression  \\\n",
       "0                1        2            150           0           2.3   \n",
       "1                0        2            108           1           1.5   \n",
       "2                0        2            129           1           2.6   \n",
       "3                0        0            187           0           3.5   \n",
       "4                0        2            172           0           1.4   \n",
       "\n",
       "  Peak_ST_seg major_vessels thal  has_heart_disease  \n",
       "0           3             0    6                  0  \n",
       "1           2             3    3                  2  \n",
       "2           2             2    7                  1  \n",
       "3           3             0    3                  0  \n",
       "4           1             0    3                  0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pandas\n",
    "# now lets look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the R Kernel for use in iPython Notebook\n",
    "\n",
    "###### Using Anaconda, it is relatively simple to install the R kernel for iPython Notebook.  This is done using the following command from any console window:\n",
    "\n",
    "conda install -c r r-essentials\n",
    "\n",
    "This actually uses anaconda's r channel and searches for the package r-essentials to install.  The r-essentials package includes the IRKernel and over 80 of the most used R packages for data science, including dplyr, shiny, ggplot2, tidyr,caret, nnet, and many others!    \n",
    "\n",
    "###### To avoid naming conflicts, every R package avaiable  within Anaconda's r channel uses the same name as its corresponding R library, except prefixed with \"r-\".  For example, if you would normally access a package in R using the following library command:\n",
    "\n",
    "library(SparkR)\n",
    "\n",
    "###### You would use the following command to install this package using conda: \n",
    "\n",
    "conda install -c r r-SparkR\n",
    "\n",
    "You will see some examples of this later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-7011debbce86>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#setwd(\"D:\\\\SMU\\\\Larson\\DataMiningClass\\\\2U_DataMining\\\\Jakes Notebooks\\data\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputData\u001b[0m \u001b[1;33m<\u001b[0m\u001b[1;33m-\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/heart_disease.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'inputData' is not defined"
     ]
    }
   ],
   "source": [
    "# now let's read in the same data using R and then save it as a dataframe\n",
    "\n",
    "# set the working directory (this may come in handy sometimes when using R or R Studio)\n",
    "#setwd(\"D:\\\\SMU\\\\Larson\\DataMiningClass\\\\2U_DataMining\\\\Jakes Notebooks\\data\")  \n",
    "\n",
    "df = inputData <- read.csv(\"data/heart_disease.csv\",sep = \",\", header = T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets look at the data using R\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# now let's a get a summary of the variables using Pandas\n",
    "print df.info()\n",
    "# we can see that most of the data \n",
    "#  is saved as an integer or as a nominal object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# now let's a get a summary of the variables using R\n",
    "str(df)\n",
    "# We can also get additional variable information using summary() \n",
    "summary(df)\n",
    "# we can see that most of the data \n",
    "#  is saved as an integer or as a factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has been read into working memory and is known as a DataFrame.\n",
    "\n",
    "### Reading Data from SQLite3 with Pandas\n",
    "We can also connect to a sqlite3 database using the built in sqlite3 package that ships with python. This data will be read into working memory and is known as a DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# but csv files are not the only thing we can work with\n",
    "# what if the data was actually in a sqlite database?\n",
    "del df\n",
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect('data/heart_disease_sql') # again this file is in the same directory\n",
    "df = pd.read_sql('SELECT * FROM heart_disease', con)  # the table name is heart_disease\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "# notice now, however, that the data types are all objects!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing R Packages for use in R, R Studio, and iPython Notebook\n",
    "\n",
    "###### When using R or R Studio, the following commands will first install and then import any available package using R:\n",
    "\n",
    "install.packages(\"RSQLite\") # packages only need to be installed one time!\n",
    "\n",
    "library(\"RSQLite\") # the library() command imports the package from that point on. \n",
    "\n",
    "###### When using iPython Notebook however, R packages must installed in a slightly different manner.  While many packages are included with r-essentials, you will eventually run into a package such as RSQLite which is missing.  When this happens, you can install them using the following conda syntax from any command prompt:\n",
    "\n",
    "conda install -c r r-RSQLite\n",
    "\n",
    "###### Notice that \"r-\" is appended to the R package name.  Here is an example of what this would look like in windows:\n",
    "\n",
    "<img src=\"condaRpackageInstall.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reading Data from SQLite3 with R using the RSQLite Package\n",
    "We can also connect to a sqlite3 database using the built in RSQLite package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# but csv files are not the only thing we can work with\n",
    "# what if the data was actually in a sqlite database?\n",
    "\n",
    "#Only install one time!\n",
    "#conda install -c r r-RSQLite\n",
    "\n",
    "library(\"RSQLite\")\n",
    "\n",
    "# connect to the sqlite file\n",
    "con <- dbConnect(RSQLite::SQLite(),dbname=\"data/heart_disease_sql\")\n",
    "df <- dbGetQuery(con,'SELECT * FROM heart_disease')\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inspect the fields using R\n",
    "str(df)\n",
    "# notice now, however, that the data types are all chr!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Working with DataFrames using Pandas and R\n",
    " We can index into a DataFrame in a number of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# the variable names are embedded into the structure\n",
    "print df.age\n",
    "print df['age'] # but can also be accessed using strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================\n",
    "#R\n",
    "# the variable names are embedded into the structure but accessed using a $ character\n",
    "df$age\n",
    "df['age'] # but can also be accessed using strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "print df.chest_pain.min(), df.chest_pain.max(), df.chest_pain.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "paste(min(df$chest_pain), max(df$chest_pain), mean(df$chest_pain), sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# lets get rid of the 'site' variable\n",
    "if 'site' in df:\n",
    "    del df['site']\n",
    "\n",
    "print df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# lets get rid of the 'site' variable using R\n",
    "#df[ SUBSET ROWS HERE , SUBSET COLUMNS HERE]\n",
    "#See the follwing URL for a million different alternate examples:\n",
    "#    http://stackoverflow.com/questions/4605206/drop-data-frame-columns-by-name\n",
    "df <- df[, !(colnames(df) %in% c(\"site\"))] \n",
    "\n",
    "#do the same thing using the column index, CAREFUL, IF you run this twice, age will be gone as well!\n",
    "#This is litterally saying select all columns, except the column at index 1\n",
    "#df <- df[, -1] \n",
    "\n",
    "str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# Notice that all of the data is stored as a non-null object\n",
    "# That's not good. It means we need to change those data types\n",
    "# in order to encode the variables properly. Right now Pandas\n",
    "# thinks all of our variables are nominal!\n",
    "\n",
    "import numpy as np\n",
    "# replace '?' with -1, we will deal with missing values later\n",
    "df = df.replace(to_replace='?',value=-999) \n",
    "\n",
    "# let's start by first changing the numeric values to be floats\n",
    "continuous_features = ['rest_blood_press', 'cholesterol', \n",
    "                       'max_heart_rate', 'ST_depression']\n",
    "\n",
    "# and the oridnal values to be integers\n",
    "ordinal_features = ['age','major_vessels','chest_pain',\n",
    "                    'rest_ecg','Peak_ST_seg','thal','has_heart_disease']\n",
    "\n",
    "# we won't touch these variables, keep them as categorical\n",
    "categ_features = ['is_male','high_blood_sugar','exer_angina'];\n",
    "\n",
    "# use the \"astype\" function to change the variable type\n",
    "df[continuous_features] = df[continuous_features].astype(np.float64)\n",
    "df[ordinal_features] = df[ordinal_features].astype(np.int64)\n",
    "\n",
    "df.info() # now our data looks better!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===========================================================\n",
    "#R\n",
    "# Notice that all of the data is stored as character vectors.\n",
    "# That's not good. It means we need to change those data types\n",
    "# in order to encode the variables properly. Right now R\n",
    "# thinks all of our variables are nominal!\n",
    "\n",
    "# replace '?' with -999, we will deal with missing values later\n",
    "df[df == '?'] <- -999\n",
    "\n",
    "# let's start by first changing the numeric values to be floats\n",
    "continuous_features = c('rest_blood_press', 'cholesterol', \n",
    "                       'max_heart_rate', 'ST_depression')\n",
    "\n",
    "# and the oridnal values to be integers\n",
    "ordinal_features = c('age','major_vessels','chest_pain',\n",
    "                    'rest_ecg','Peak_ST_seg','thal','has_heart_disease')\n",
    "\n",
    "# we won't touch these variables, keep them as categorical\n",
    "categ_features = c('is_male','high_blood_sugar','exer_angina')\n",
    "\n",
    "# use the sapply function to change the variable type\n",
    "df[ , continuous_features] <- lapply(df[,continuous_features],as.numeric)\n",
    "df[ , ordinal_features] <- lapply(df[,ordinal_features],as.numeric)\n",
    "df[ , categ_features] <- lapply(df[,categ_features],as.factor)\n",
    "\n",
    "str(df) # now our data looks better!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get summary of all attributes in the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas\n",
    "df.describe() # will get summary of continuous or the nominals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "summary(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 920 entries in this data frame. Notice that this data frame has a number of missing values denoted by the value -999 (that we changed the '?' value to before). We need to either remove the missing values from the dataset OR we need to fill in with our best guess for those values. Let's first drop all the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many value have the -1 (which we set as the missing values) \n",
    "import numpy as np\n",
    "\n",
    "# let's set those values to NaN, so that Pandas understand they are missing\n",
    "df = df.replace(to_replace=-999,value=np.nan) # replace -1 with NaN (not a number)\n",
    "print df.info()\n",
    "df.describe() # scroll over to see the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# how many values were previously ? (which we set as the missing values) \n",
    "length(df[df == '-999'])  #Count the  -999 values\n",
    "\n",
    "# let's set those values to NA, so that R understand they are missing\n",
    "df[df == '-999'] <- NA\n",
    "\n",
    "# how many values were previously -999 (which we set as the missing values) \n",
    "sum(is.na(df))\n",
    "\n",
    "str(df)\n",
    "summary(df) # scroll over to see the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. Notice how the number of attributes went down in the description function. Looks like we need to impute values. If we drop the rows with missing data, we will be throwing away almost 80% of the data collected. No way!!\n",
    "\n",
    "### Imputation of NaN values (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# lets look at some stats of the data\n",
    "df.median() # only calculates for numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# lets look at some stats of the data\n",
    "summary(df) # only calculates for numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# the 'fillna' function will take the given series (the output above)\n",
    "# and fill in the missing values for the columns it has\n",
    "df_imputed = df.fillna(df.median()) # note that to do this all values must be numeric\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the object variables are unchanged, but all the numeric/ordinal values have been filled in with the median of the columns. Let's try something (slightly) smarter, and fill in the oridinals with the median and the continuous with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#R\n",
    "# the lapply function will take the given series (the output above)\n",
    "# and fill in the missing values for the columns it has\n",
    "\n",
    "df_imputed <- df\n",
    "\n",
    "#Get only the numeric columns in the data frame\n",
    "numCols <- sapply(df, is.numeric)\n",
    "\n",
    "#Now perform Simple Mean Imputation on each numeric column\n",
    "df_imputed[,numCols] <- lapply(df_imputed[,numCols], function(x) { \n",
    "  x[is.na(x)] <- mean(x, na.rm = TRUE)\n",
    "  x\n",
    "})\n",
    "\n",
    "summary(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice all of the NA values are now imputed / replaced with the mean() for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas\n",
    "# make  one series for imputing with\n",
    "series_mean = df[continuous_features].mean()\n",
    "series_median = df[categ_features+ordinal_features].median()\n",
    "cat_series = pd.concat((series_median,series_mean))\n",
    "\n",
    "print cat_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "# now let's impute the numbers a bit differently\n",
    "\n",
    "df_imputed = df.fillna(value=cat_series)\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "# impute with the mean for the continuous features\n",
    "df_imputed <- df\n",
    "\n",
    "df_imputed[,continuous_features] <- lapply(df_imputed[,continuous_features], function(x) { \n",
    "  x[is.na(x)] <- mean(x, na.rm = TRUE)\n",
    "  x\n",
    "})\n",
    "\n",
    "# impute with the median for the categ_features + ordinal_features\n",
    "df_imputed[,c(continuous_features, ordinal_features)] <- lapply(df_imputed[,c(continuous_features, ordinal_features)], function(x) { \n",
    "  x[is.na(x)] <- median(x)\n",
    "  x\n",
    "})\n",
    "\n",
    "summary(df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pandas\n",
    "df_imputed[categ_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "summary(df_imputed[, categ_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing logically into Data Frames\n",
    "Let's now say that we are only interested in the summary of the dataframe when the patient has heart disease. We can achieve this using a few line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "df_imputed[df_imputed.has_heart_disease==0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "summary(df_imputed[df_imputed$has_heart_disease==0, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# or we can use the extremely useful \"groupby\" function\n",
    "df_imputed.groupby(by='has_heart_disease').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#R\n",
    "#or we can use the extremely useful \"groupby\" function\n",
    "library(dplyr)\n",
    "numCols <- sapply(df, is.numeric)\n",
    "\n",
    "df[,numCols] %>%\n",
    "  group_by(has_heart_disease) %>%\n",
    "  summarise_each(funs(median))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "df_imputed.groupby(by=df_imputed.has_heart_disease>0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "df_imputed.groupby(by=df_imputed.major_vessels>2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding of Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "# one hot encoded variables can be created using the get_dummies variable\n",
    "tmpdf = pd.get_dummies(df_imputed['chest_pain'],prefix='chest')\n",
    "\n",
    "tmpdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas\n",
    "#one hot encoding of ALL categorical variables\n",
    "# there is lot going on in this one line of code, so let's step through it\n",
    "\n",
    "# pd.concat([*]], axis=1) // this line of code concatenates all the data frames in the [*] list\n",
    "# [** for col in categ_features] // this steps through each feature in categ_features and \n",
    "#                                //   creates a new element in a list based on the output of **\n",
    "# pd.get_dummies(df_imputed[col],prefix=col) // this creates a one hot encoded dataframe of the variable=col (like code above)\n",
    "\n",
    "one_hot_df = pd.concat([pd.get_dummies(df_imputed[col],prefix=col) for col in categ_features], axis=1)\n",
    "\n",
    "one_hot_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling R from iPython\n",
    "\n",
    "- Note: you will need R installed on your machine to run these!!\n",
    "\n",
    "iPython has a lot of interesting \"magics\" built in. If you use R and have it installed on your machine, then you can write and look at R code directly from iPython cells. R also uses data frames, which we can push data into directly from the Pandas object we are using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT PANDAS DATAFRAME TO R DATA.FRAME\n",
    "# adapted from: http://tagteam.harvard.edu/hub_feeds/1981/feed_items/196017\n",
    "# I have better luck with both calls here\n",
    "\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython\n",
    "\n",
    "df_colnames = df_imputed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_colnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take the data frame from pandas and tell Rmagics that we want to have variables available in the R workspace. We use the %%R command to tell iPython that the entire cell is R code. The \"-i\" tells Rmagics that we want to transfer those variables over to R.\n",
    "\n",
    "The following code will take the variables df_imputed and df_colnames into the R workspace and test if they are truly saved as R data.frames type variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_imputed,df_colnames \n",
    "\n",
    "colnames(df_imputed) <- unlist(df_colnames); \n",
    "print(is.data.frame(df_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They were data.frames! Great. Let's call an R function on the data.frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i df_imputed \n",
    "print(summary(df_imputed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are able to call some R and get console output, now let's make some changes to the data.fram in R and print the result back in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'original:', df_imputed.age.head()\n",
    "\n",
    "# give df_imputed, then multiply it by to in R\n",
    "# the %R command tells iPython its just one line of R code\n",
    "%R -i df_imputed df_imputed$age <- df_imputed$age*2\n",
    "\n",
    "# now we are back in python, did it change?\n",
    "print 'after manipulation in R:', df_imputed.age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it looks like the data was not synchronized... So instead let's setup an output variable for the DataFrame that we send into R. `-i df_imputed` means that we are sending in the DataFrame as an R data.frame. `-o df_imputed` means we are also getting the same variable and copying it back to the python workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'original:', df_imputed.age.head() \n",
    "\n",
    "# This is the same code as before, but now with an output variable\n",
    "%R -i df_imputed -o df_imputed  df_imputed$age <- df_imputed$age*2\n",
    "# you can place the above on any line to make sure that the data stays\n",
    "# synchronized between pandas and python\n",
    "print 'after manipulation in R:', df_imputed.age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. So now we can send DataFrames into R, manipulate them, and get them back into the python workspace. Is this memory hogging? Yes. Is it really useful for when you want to connect and work with different parts of R? You betcha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also just go and get new variables from R and \n",
    "# have them spit them back out for us\n",
    "# here I am sending in df_imputed and getting back a data frame\n",
    "# created in R\n",
    "%R -i df_imputed -o df_from_R df_from_R <- df_imputed\n",
    "\n",
    "# notice that the only differebce is that the integers are 32 bits\n",
    "df_from_R.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Use this as a reference sheet for Pandas, some basic imputation, and calling R code. Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
